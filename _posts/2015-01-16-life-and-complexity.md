---
layout: post
title: Life and Complexity
tags: [dev, life, complexity]
---
### Life & Complexity
When embracing life in it's whole with all it's aspects, or any subset of it, one should always embrace complexity too. The reality we live in is getting more complex every instant we experience and even now we can observe it prevail over simplicity. If you are not careful it's easy to believe things are getting simpler or easier and this is really no wonder because in many cases it _is_ indeed easier for the average person to obtain the stuff he or she needs. 

At least, if you are lucky enough to live in a country where your leaders are not too corrupt (and/or evil, hungry for power, etc.). Corruptness of people who end up in powerful positions is a whole other topic so I'll leave it at this for now.

#### Sidenote
This is a long post. I will go rambling from east to west, make a lot of assumptions, be ignorant and a whole lot of other stuff... Maybe... Probably.

### Entropy
At this point the author has to concede he is not much of a thermal dynamics expert nor possibly aware from all the other fields that deal with a concept known as _entropy_. It's a big word but in this case I'm referring to the amount of _randomness_ there is in any kind of system ([entropy: order and disorder)](http://en.wikipedia.org/wiki/Entropy_(order_and_disorder).

And even though in software we don't deal with entropy directly. I do consider force to battle against and as a programmer it would be wise to recognize entropy if you have the chance.

#### Sidenote
If you really look at how entropy is used (even though it might have a special definition) in other systems you will notice it will always have to do with the amount of _disorder_ there is a system. How a particular form of _disorder_ is defined is usually a parameter of the field in which the concept of entropy is used.

### Disorder
I've talked a bit about disorder but what does it really mean? Well if we are talking about entropy in _bits_ which is what I'm accustomed to then it just means the amount of _randomness_ between the ones and zeros. I have no clue about the actual calculations at this point but I will just define what I'm talking about in terms I (and hopefully everyone else understands). Without the fancy symbols.

TODO: Add more insightful information about how to actually measure and calculate entropy.

### With Some Bits
__DISCLAIMER__: I'm totally going out on a limb and assume things (like that I know what I'm talking about so please always do your own research. I need to research them later but for now I just need to get these words out sorry if you are mislead but please be warned by this disclaimer).

Let's go with bits because they are easy to work with (that's why computers like them so much but that's another story). We can assume a bit to be either zero (<code>0</code>) or one (<code>1</code>). Now let's assume we have one bit.

The bit will have zero entropy. Because it's a bit. It's the point from which we are observing the system. It doesn't matter if it is zero or one. It will have zero entropy because it is consistent from our perspective.

Let's have 10 bits. Five of those bits are 0 and the other five are 1. Now we have a bit of entropy but how much? If we only inspect those 10 bits and no more we have fifty percent of entropy (I guess? Spock says it seems logical). 

### With Infinite Bits
Now what if we are observing an infinite stream of bits that is somehow converted/produced by reality itself. What kinds of patters would we see? How could we calculate it's entropy? Well it seems like (from my casual observations) that entropy in any kind of system is increased by feeding energy into it. It seems the global consensus (Wikipedia) agrees (emphasis mine):

> Entropy is an extensive property. It has the _dimension of energy divided by temperature_, which has a [bla about units and convertions].

This basically says (to me at least but feel free to flame/correct me if I'm wrong) that entropy increases if you increase energy relative to the temperature (if temperature stays equals, if you increase energy you gain entropy).

#### Flying With Entropy
It's easy to make entropy. There is a whole lot of ways but one of the most fundamental ways has to be one of the oldest: creating a fire.

Why do airplanes fly? Why do hot air balloons float? Some people might say "because of their wing shape and design" and in the case of balloons people might say "because they heated the air silly!" (if they are in jolly mood, some people might also punch you in the face but that's another story).

Airplanes fly because of entropy. Hot air balloons float because of entropy. But entropy is meaningless unless we provide a context where it applies. In this case it's the molecules that form our atmosphere.

